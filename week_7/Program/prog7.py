"""
Name:  Arterio Rodrigues
Email: arterio.rodrigues47@myhunter.cuny.edu
Resources:  Python3.8 Pandas
"""
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import LassoCV, RidgeCV, LinearRegression
from sklearn.model_selection import train_test_split

import pickle
import pandas as pd
import numpy as np

def import_data(csv_file):
    '''
    This function takes one input:
    csv_file: the name of a CSV file time series data for commodities from FRED.
    The data in the file is read into a DataFrame, and a new column, units, that indexes 
    the lines of the files and represents the number of units (e.g. months, weeks, years). 
    It can be generated by looping through the dataset or using the index as a column 
    (e.g. df.index.to_series()). The resulting DataFrame is returned.
    '''

    df = pd.read_csv(csv_file)
    df['units'] = df.index.to_series()

    return df

def split_data(df, y_col_name, test_size = 0.25, random_state = 21):
    '''
    This function takes 4 input parameters:
    df: a DataFrame containing with a columns units.
    y_col_name: the name of the column of the dependent variable.
    test_size: accepts a float between 0 and 1 and represents the proportion of the data set 
    to use for training. This parameter has a default value of 0.25.
    random_state: Used as a seed to the randomization. This parameter has a default value of 
    1870.
    Returns the data split into 4 subsets, corresponding to those returned by train_
    test_split: x_train, x_test, y_train, and y_test. where units is the "x" column and 
    the input parameter, y_col_name is the "y" column.
    Note: this is function is very similar to the splitting of data into training and 
    testing sets from Program 6.
    '''

    x_train, y_train, x_test , y_test = train_test_split(
        df['units'], df[y_col_name], test_size = test_size, random_state = random_state)

    return x_train, y_train, x_test , y_test

def fit_poly(xes, yes, epsilon = 100, verbose = False):
    '''
    This function takes four inputs:
    xes: a DataFrame that includes the column units.
    yes: a series of the same length as xes.
    epsilon: the size of the sample. It has a default value of 100.
    verbose: when True, prints out the MSE cost for each degree tried (in format: 
    f'MSE cost for deg {deg} poly model: {error:.3f}' for degrees 1, 2, ..., until
      the error is below epsilon, see example below). It has a default value of False.
    It returns the smallest integer degree >= 1 for which the model yields a MSE of 
    less than the specified epsilon and the coefficients as a vector for df["units"] and 
    df[y_col]. If it does not find a model with an error less than epsilon by degree 5, 
    returns None. When fitting the linear regression model, the fit_intercept=False. 
    Hint: see the Chapter 16 for examples of using PolynomialFeatures().
    '''

    poly = PolynomialFeatures(degree = 5, include_bias = False)
    poly_features = poly.fit_transform(xes)

    degrees = [1, 2, 3, 4, 5]

    models = [LinearRegression().fit(poly_features[ :, :deg], yes) for deg in degrees]

    for index, mod in enumerate(models):
        prediction = mod.predict(poly_features[:, :degrees[index]])
        error = mean_squared_error(yes , prediction)

        if verbose == True:
            print(f'MSE cost for deg {degrees[index]} poly model: {error:.3f}')
        if error < epsilon:
            return mod
    return None

def fit_model(xes, yes, poly_deg = 2, reg = 'lasso'):
    '''
    xes: a series of numeric values.
    yes: a series of numeric values.
    poly_deg: the degree of the polynomial features to be created. It has a default value of 
    2.
    reg: The type of regularization used: ridge or lasso. It has a default value of lasso.
    This function fits a model with polynomial features using Lasso or Ridge regression with
      cross validation:
    Apply PolynomialFeatures to the xes with degree equal to poly_deg.
    If reg equals ridge, use RidgeCV to instantiate and fit a model to the polynomial
    features and yes. Otherwise, use LassoCV to to instantiate and fit the model.
    Returns the model as serialized object (i.e. a pickled object).
    '''

    poly = PolynomialFeatures(degree = poly_deg, include_bias = False)
    poly_features = poly.fit_transform(xes.array.reshape(-1, 1))

    if reg == 'lasso':
        mod = LassoCV().fit(poly_features, yes)
    else:
        mod = RidgeCV().fit(poly_features, yes)

    ser_mod = pickle.dumps(mod)

    return ser_mod

def predict_using_trained_model(model_pkl, poly_xes, yes):
    '''
    This function takes three inputs:
    mod_pkl: a trained model for the data, stored in pickle format.
    poly_xes: an array or DataFrame of numeric columns with no null values.
    yes: an array or DataFrame of numeric columns with no null values.
    Computes and returns the mean squared error and r2 score between the values predicted 
    by the model (mod on x) and the actual values (y). Note that sklearn.metrics contains 
    two functions that may be of use: mean_squared_error and r2_score.
    '''
    mod_reconstructed = pickle.loads(model_pkl)

    predicted_values = mod_reconstructed.predict(poly_xes)

    mse = mean_squared_error(yes, predicted_values)
    r2s = r2_score(yes, predicted_values)

    return mse, r2s
